{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python imports\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn import neighbors, datasets, feature_selection\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "\n",
    "import collections\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "iris_df['Target'] = iris.target"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Implement KNN classification, using the sklearn package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10b8375d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2QBwsDczGwD/H176Y0AZFsBgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b7fcf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split data and target classifications into train and test sets and run KNN classification\n",
    "def knn(data, target, n_neighbors):\n",
    "\n",
    "    # split the data into train and test datasets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=12)\n",
    "\n",
    "    # Loop through each K value, run KNN classification, and append the scores to the output\n",
    "    scores = []\n",
    "    for n in n_neighbors:\n",
    "        clf = neighbors.KNeighborsClassifier(n)\n",
    "        clf.fit(x_train, y_train)\n",
    "        scores.append(clf.score(x_test, y_test))\n",
    "        \n",
    "    return scores\n",
    "\n",
    "# use odd for K from 1 to 51\n",
    "n_neighbors = range(1, 51, 2)\n",
    "\n",
    "# get the scores\n",
    "scores = knn(iris.data, iris.target, n_neighbors)\n",
    "\n",
    "# plot result as K vs score\n",
    "plt.plot(n_neighbors, scores, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. Use the sklearn package to implement cross-validation for your classifier. Use 5 folds for your cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96666667  0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.96\n",
      "[ 0.96666667  0.96666667  0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.96666667  1.          0.93333333  0.96666667  1.        ]\n",
      "0.973333333333\n",
      "[ 0.96666667  1.          0.96666667  0.96666667  1.        ]\n",
      "0.98\n",
      "[ 0.96666667  1.          0.96666667  0.93333333  1.        ]\n",
      "0.973333333333\n",
      "[ 0.93333333  1.          1.          0.96666667  1.        ]\n",
      "0.98\n",
      "[ 0.93333333  1.          0.96666667  0.96666667  1.        ]\n",
      "0.973333333333\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.93333333  1.        ]\n",
      "0.96\n",
      "[ 0.93333333  0.96666667  0.93333333  0.96666667  1.        ]\n",
      "0.96\n",
      "[ 0.9         0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.946666666667\n",
      "[ 0.9         0.96666667  0.9         0.9         1.        ]\n",
      "0.933333333333\n",
      "[ 0.9         0.96666667  0.9         0.9         1.        ]\n",
      "0.933333333333\n",
      "[ 0.93333333  0.96666667  0.9         0.9         1.        ]\n",
      "0.94\n",
      "[ 0.93333333  0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.953333333333\n",
      "[ 0.9         0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.946666666667\n",
      "[ 0.9         0.93333333  0.93333333  0.93333333  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.93333333  0.93333333  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.93333333  0.93333333  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.9         0.96666667  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.86666667  0.96666667  0.96666667]\n",
      "0.926666666667\n",
      "[ 0.9         0.93333333  0.9         0.96666667  1.        ]\n",
      "0.94\n"
     ]
    }
   ],
   "source": [
    "# run 5-fold cross-validation on the input and targets and return the scores\n",
    "def xvalidate(data, target, n_neighbor, n_folds):\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbor, weights='uniform')\n",
    "    clf.fit(data, target)\n",
    "    scores = cross_val_score(clf, data, target, cv=n_folds)\n",
    "    return scores\n",
    "\n",
    "# simple util method for getting the mean value of a list\n",
    "def mean(values):\n",
    "    tot = 0\n",
    "    for v in values:\n",
    "        tot += v\n",
    "    return tot / len(values)\n",
    "\n",
    "# run cross-validation using a series of K values\n",
    "for x in range(1, 51, 2):\n",
    "    xvalidate_scores = xvalidate(iris.data, iris.target, x, 5)\n",
    "    print xvalidate_scores\n",
    "    print xvalidate_scores.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Use your KNN classifier and cross-validation code from (1) and (2) above to determine the optimal value of K (number of nearest neighbors to consult) for this Iris dataset. Hint: This hyperparameter will be a number between 1 and 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "neighbor scores {1: 0.95999999999999996, 2: 0.94666666666666655, 3: 0.96666666666666679, 4: 0.97333333333333338, 5: 0.97333333333333338, 6: 0.98000000000000009, 7: 0.98000000000000009, 8: 0.96666666666666679, 9: 0.97333333333333338, 10: 0.98000000000000009, 11: 0.98000000000000009, 12: 0.98000000000000009, 13: 0.97333333333333338, 14: 0.96666666666666656, 15: 0.96666666666666679, 16: 0.96666666666666679, 17: 0.96666666666666679, 18: 0.96666666666666656, 19: 0.96666666666666679, 20: 0.95999999999999996, 21: 0.96666666666666679, 22: 0.95999999999999996, 23: 0.95999999999999996, 24: 0.94666666666666666, 25: 0.95999999999999996, 26: 0.94666666666666655, 27: 0.94666666666666666, 28: 0.93999999999999984, 29: 0.93333333333333324, 30: 0.93999999999999984, 31: 0.93333333333333324, 32: 0.94666666666666655, 33: 0.93999999999999984, 34: 0.95333333333333337, 35: 0.95333333333333337, 36: 0.95333333333333337, 37: 0.94666666666666666, 38: 0.93333333333333335, 39: 0.94000000000000006, 40: 0.92666666666666675, 41: 0.94000000000000006, 42: 0.94000000000000006, 43: 0.94000000000000006, 44: 0.92666666666666675, 45: 0.94000000000000006, 46: 0.93333333333333335, 47: 0.92666666666666675, 48: 0.92666666666666675, 49: 0.94000000000000006, 50: 0.91333333333333344, 51: 0.92000000000000015, 52: 0.91333333333333344, 53: 0.91333333333333344, 54: 0.90000000000000013, 55: 0.90666666666666684, 56: 0.89333333333333331, 57: 0.89333333333333331, 58: 0.88666666666666671, 59: 0.88666666666666671, 60: 0.88666666666666671, 61: 0.88666666666666671, 62: 0.88666666666666671, 63: 0.88666666666666671, 64: 0.88666666666666671, 65: 0.88666666666666671, 66: 0.88000000000000012, 67: 0.88666666666666671, 68: 0.88666666666666671, 69: 0.88666666666666671, 70: 0.88000000000000012, 71: 0.88666666666666671, 72: 0.89333333333333331, 73: 0.89333333333333331, 74: 0.89333333333333353}\n",
      "optimal K = 12\n"
     ]
    }
   ],
   "source": [
    "print len(iris.data)\n",
    "\n",
    "# find the optimal K which is the highest value of K with the best score\n",
    "# determined by 5-fold cross-validation on a list of odd-numbered K from 1 to 51\n",
    "def find_optimal_k(data):\n",
    "    max_neighbors = int(len(data) / 2)\n",
    "    neighbor_scores = {}\n",
    "    for x in range(1, max_neighbors, 1):\n",
    "        xvalidate_scores = xvalidate(iris.data, iris.target, x, 5)        \n",
    "        avg = mean(xvalidate_scores)\n",
    "        neighbor_scores[x] = avg\n",
    "    max_value = max(neighbor_scores.values())\n",
    "    indices = [i for i, x in enumerate(neighbor_scores.values()) if x == max_value]\n",
    "    max_index = indices[-1] + 1 # add 1 from 0-based indexing\n",
    "    return max_index, neighbor_scores\n",
    "\n",
    "k, neighbor_scores = find_optimal_k(iris.data)\n",
    "print \"neighbor scores\", neighbor_scores\n",
    "print \"optimal K =\", k\n",
    "# can use np.argmax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Using matplotlib, plot classifier accuracy versus the hyperparameter K for a range of K that you consider interesting. Explain in words what you are seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10bc556d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMxsQ/x/x/l5k7HPIyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bb9d450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(neighbor_scores.keys(), neighbor_scores.values())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There is a \"tie\" for the optimal value of K between 6 to 12.  Several K values in this range scored the same (at about 0.98).  The optimal value of K is 12 if we consider \"optimal\" to be the highest value of K with the highest score (in case of a tie).  As K is increased and more neighbors are added to the classication, the accuracy decreases, due to the majority classification value tending to dominate as locality decreases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Now, write your own implementation of cross-validation in Python without using the cross-validation methods from sklearn. Cross validation is a very important concept. Implementing it yourself in Python is the best way to learn and understand it. Compare the results of your cross-validation code with your results using the cross-validation in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin_index 0\n",
      "end_index 29\n",
      "fold_indices [80, 49, 41, 115, 122, 50, 138, 91, 110, 134, 74, 43, 128, 31, 116, 137, 144, 117, 83, 19, 69, 2, 51, 127, 88, 47, 147, 46, 8]\n",
      "fold_data [[ 5.5  2.4  3.8  1.1]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 6.4  3.2  5.3  2.3]\n",
      " [ 7.7  2.8  6.7  2. ]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.   3.   4.8  1.8]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 6.5  3.2  5.1  2. ]\n",
      " [ 6.1  2.6  5.6  1.4]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 5.   3.5  1.6  0.6]\n",
      " [ 6.4  2.8  5.6  2.1]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 6.5  3.   5.5  1.8]\n",
      " [ 6.4  3.1  5.5  1.8]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 7.7  3.8  6.7  2.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 6.1  3.   4.9  1.8]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]]\n",
      "fold_labels [1 0 0 2 2 1 2 1 2 2 1 0 2 0 2 2 2 2 1 0 1 0 1 2 1 0 2 0 0]\n",
      "fold_score [0.88888888888888884]\n",
      "begin_index 30\n",
      "end_index 59\n",
      "fold_indices [30, 0, 130, 94, 33, 36, 109, 28, 73, 38, 55, 9, 26, 146, 35, 4, 141, 81, 86, 126, 64, 104, 13, 79, 98, 93, 23, 87, 107]\n",
      "fold_data [[ 4.8  3.1  1.6  0.2]\n",
      " [ 5.1  3.5  1.4  0.2]\n",
      " [ 7.4  2.8  6.1  1.9]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 7.2  3.6  6.1  2.5]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.5  2.4  3.7  1. ]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.2  2.8  4.8  1.8]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 7.3  2.9  6.3  1.8]]\n",
      "fold_labels [0 0 2 1 0 0 2 0 1 0 1 0 0 2 0 0 2 1 1 2 1 2 0 1 1 1 0 1 2]\n",
      "fold_score [0.0]\n",
      "begin_index 60\n",
      "end_index 89\n",
      "fold_indices [90, 121, 11, 125, 95, 57, 62, 85, 24, 40, 133, 17, 136, 32, 39, 7, 113, 37, 120, 67, 124, 140, 22, 96, 1, 75, 82, 132, 108]\n",
      "fold_data [[ 5.5  2.6  4.4  1.2]\n",
      " [ 5.6  2.8  4.9  2. ]\n",
      " [ 4.8  3.4  1.6  0.2]\n",
      " [ 7.2  3.2  6.   1.8]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.   2.2  4.   1. ]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 6.3  2.8  5.1  1.5]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 5.7  2.5  5.   2. ]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 6.9  3.2  5.7  2.3]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 6.7  3.1  5.6  2.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 6.6  3.   4.4  1.4]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.4  2.8  5.6  2.2]\n",
      " [ 6.7  2.5  5.8  1.8]]\n",
      "fold_labels [1 2 0 2 1 1 1 1 0 0 2 0 2 0 0 0 2 0 2 1 2 2 0 1 0 1 1 2 2]\n",
      "fold_score [0.88888888888888884]\n",
      "begin_index 90\n",
      "end_index 119\n",
      "fold_indices [149, 5, 101, 16, 20, 68, 97, 71, 129, 56, 59, 15, 66, 119, 21, 76, 143, 65, 123, 34, 72, 114, 6, 29, 60, 100, 70, 99, 44]\n",
      "fold_data [[ 5.9  3.   5.1  1.8]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 7.2  3.   5.8  1.6]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.6  3.   4.5  1.5]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 6.3  2.7  4.9  1.8]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 5.8  2.8  5.1  2.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 6.3  3.3  6.   2.5]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 5.1  3.8  1.9  0.4]]\n",
      "fold_labels [2 0 2 0 0 1 1 1 2 1 1 0 1 2 0 1 2 1 2 0 1 2 0 0 1 2 1 1 0]\n",
      "fold_score [0.44444444444444442]\n",
      "begin_index 120\n",
      "end_index 149\n",
      "fold_indices [106, 14, 142, 27, 52, 103, 18, 45, 118, 12, 145, 78, 139, 112, 135, 54, 63, 25, 131, 48, 92, 53, 102, 84, 89, 77, 111, 61, 148]\n",
      "fold_data [[ 4.9  2.5  4.5  1.7]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 6.9  3.1  4.9  1.5]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 4.8  3.   1.4  0.3]\n",
      " [ 7.7  2.6  6.9  2.3]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 6.9  3.1  5.4  2.1]\n",
      " [ 6.8  3.   5.5  2.1]\n",
      " [ 7.7  3.   6.1  2.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.   3.   1.6  0.2]\n",
      " [ 7.9  3.8  6.4  2. ]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 7.1  3.   5.9  2.1]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 6.7  3.   5.   1.7]\n",
      " [ 6.4  2.7  5.3  1.9]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 6.2  3.4  5.4  2.3]]\n",
      "fold_labels [2 0 2 0 1 2 0 0 2 0 2 1 2 2 2 1 1 0 2 0 1 1 2 1 1 1 2 1 2]\n",
      "fold_score [0.44444444444444442]\n",
      "[0.88888888888888884, 0.0, 0.88888888888888884, 0.44444444444444442, 0.44444444444444442]\n",
      "0.533333333333\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def my_xvalidate(data, target, folds, n_neighbors):\n",
    "    \n",
    "    # make a list of the indices in the dataset\n",
    "    data_indices = range(0, len(data), 1)\n",
    "    \n",
    "    #print data_indices\n",
    "    \n",
    "    # shuffle the indices\n",
    "    random.shuffle(data_indices)\n",
    "    \n",
    "    # get the number of items in each fold\n",
    "    n_items = int((len(data) / folds))\n",
    "    \n",
    "    # start index for fold\n",
    "    begin_index = int(0)\n",
    "            \n",
    "    # end index for fold\n",
    "    end_index = int(n_items - 1)\n",
    "                \n",
    "    scores = []\n",
    "    \n",
    "    # iterate over the folds\n",
    "    for n_fold in range(folds):\n",
    "        \n",
    "        # get the indices for this fold\n",
    "        fold_indices = data_indices[begin_index:end_index]\n",
    "        \n",
    "        # get the data for this fold\n",
    "        fold_data = data[fold_indices]\n",
    "                        \n",
    "        # get the labels for this fold\n",
    "        fold_labels = target[fold_indices]\n",
    "        \n",
    "        # score the fold\n",
    "        fold_score = knn(fold_data, fold_labels, n_neighbors)\n",
    "        \n",
    "        print \"begin_index\", begin_index\n",
    "        print \"end_index\", end_index        \n",
    "        print \"fold_indices\", fold_indices\n",
    "        print \"fold_data\", fold_data\n",
    "        print \"fold_labels\", fold_labels\n",
    "        print \"fold_score\", fold_score\n",
    "        \n",
    "        # append the score for this fold\n",
    "        scores.append(fold_score[0])\n",
    "        \n",
    "        # increment begin and end indices for next fold        \n",
    "        begin_index += n_items\n",
    "        end_index += n_items    \n",
    "    \n",
    "    return scores\n",
    "    \n",
    "# run cross-validation on all iris data using 5-fold cross-validation with K = 11\n",
    "scores = my_xvalidate(iris.data, iris.target, 5, [11])\n",
    "print scores\n",
    "print mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_indices = range(0, len(iris_df), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td> 7.6</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 6.6</td>\n",
       "      <td> 2.1</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75 </th>\n",
       "      <td> 6.6</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 4.4</td>\n",
       "      <td> 1.4</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89 </th>\n",
       "      <td> 5.5</td>\n",
       "      <td> 2.5</td>\n",
       "      <td> 4.0</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40 </th>\n",
       "      <td> 5.0</td>\n",
       "      <td> 3.5</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18 </th>\n",
       "      <td> 5.7</td>\n",
       "      <td> 3.8</td>\n",
       "      <td> 1.7</td>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76 </th>\n",
       "      <td> 6.8</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 4.8</td>\n",
       "      <td> 1.4</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41 </th>\n",
       "      <td> 4.5</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td> 6.3</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26 </th>\n",
       "      <td> 5.0</td>\n",
       "      <td> 3.4</td>\n",
       "      <td> 1.6</td>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td> 7.2</td>\n",
       "      <td> 3.2</td>\n",
       "      <td> 6.0</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td> 7.2</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 5.8</td>\n",
       "      <td> 1.6</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78 </th>\n",
       "      <td> 6.0</td>\n",
       "      <td> 2.9</td>\n",
       "      <td> 4.5</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87 </th>\n",
       "      <td> 6.3</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 4.4</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td> 6.8</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 5.5</td>\n",
       "      <td> 2.1</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57 </th>\n",
       "      <td> 4.9</td>\n",
       "      <td> 2.4</td>\n",
       "      <td> 3.3</td>\n",
       "      <td> 1.0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93 </th>\n",
       "      <td> 5.0</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 3.3</td>\n",
       "      <td> 1.0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69 </th>\n",
       "      <td> 5.6</td>\n",
       "      <td> 2.5</td>\n",
       "      <td> 3.9</td>\n",
       "      <td> 1.1</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99 </th>\n",
       "      <td> 5.7</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 4.1</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td> 5.8</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 2.4</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92 </th>\n",
       "      <td> 5.8</td>\n",
       "      <td> 2.6</td>\n",
       "      <td> 4.0</td>\n",
       "      <td> 1.2</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38 </th>\n",
       "      <td> 4.4</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td> 6.4</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 5.6</td>\n",
       "      <td> 2.2</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4  </th>\n",
       "      <td> 5.0</td>\n",
       "      <td> 3.6</td>\n",
       "      <td> 1.4</td>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56 </th>\n",
       "      <td> 6.3</td>\n",
       "      <td> 3.3</td>\n",
       "      <td> 4.7</td>\n",
       "      <td> 1.6</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td> 5.8</td>\n",
       "      <td> 2.7</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13 </th>\n",
       "      <td> 4.3</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 1.1</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94 </th>\n",
       "      <td> 5.6</td>\n",
       "      <td> 2.7</td>\n",
       "      <td> 4.2</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td> 6.0</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 4.8</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21 </th>\n",
       "      <td> 5.1</td>\n",
       "      <td> 3.7</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td> 6.9</td>\n",
       "      <td> 3.1</td>\n",
       "      <td> 5.4</td>\n",
       "      <td> 2.1</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64 </th>\n",
       "      <td> 5.6</td>\n",
       "      <td> 2.9</td>\n",
       "      <td> 3.6</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24 </th>\n",
       "      <td> 4.8</td>\n",
       "      <td> 3.4</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44 </th>\n",
       "      <td> 5.1</td>\n",
       "      <td> 3.8</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35 </th>\n",
       "      <td> 5.0</td>\n",
       "      <td> 3.2</td>\n",
       "      <td> 1.2</td>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td> 6.1</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 4.9</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td> 6.2</td>\n",
       "      <td> 3.4</td>\n",
       "      <td> 5.4</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47 </th>\n",
       "      <td> 4.6</td>\n",
       "      <td> 3.2</td>\n",
       "      <td> 1.4</td>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96 </th>\n",
       "      <td> 5.7</td>\n",
       "      <td> 2.9</td>\n",
       "      <td> 4.2</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67 </th>\n",
       "      <td> 5.8</td>\n",
       "      <td> 2.7</td>\n",
       "      <td> 4.1</td>\n",
       "      <td> 1.0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36 </th>\n",
       "      <td> 5.5</td>\n",
       "      <td> 3.5</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td> 7.4</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 6.1</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33 </th>\n",
       "      <td> 5.5</td>\n",
       "      <td> 4.2</td>\n",
       "      <td> 1.4</td>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88 </th>\n",
       "      <td> 5.6</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 4.1</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55 </th>\n",
       "      <td> 5.7</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 4.5</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td> 6.3</td>\n",
       "      <td> 2.5</td>\n",
       "      <td> 5.0</td>\n",
       "      <td> 1.9</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td> 6.7</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 5.2</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td> 6.9</td>\n",
       "      <td> 3.1</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td> 6.4</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 5.6</td>\n",
       "      <td> 2.1</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td> 6.7</td>\n",
       "      <td> 2.5</td>\n",
       "      <td> 5.8</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62 </th>\n",
       "      <td> 6.0</td>\n",
       "      <td> 2.2</td>\n",
       "      <td> 4.0</td>\n",
       "      <td> 1.0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34 </th>\n",
       "      <td> 4.9</td>\n",
       "      <td> 3.1</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53 </th>\n",
       "      <td> 5.5</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 4.0</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td> 6.5</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 5.8</td>\n",
       "      <td> 2.2</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td> 5.9</td>\n",
       "      <td> 3.0</td>\n",
       "      <td> 5.1</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td> 7.2</td>\n",
       "      <td> 3.6</td>\n",
       "      <td> 6.1</td>\n",
       "      <td> 2.5</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31 </th>\n",
       "      <td> 5.4</td>\n",
       "      <td> 3.4</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td> 6.4</td>\n",
       "      <td> 3.1</td>\n",
       "      <td> 5.5</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td> 6.3</td>\n",
       "      <td> 2.9</td>\n",
       "      <td> 5.6</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td> 7.3</td>\n",
       "      <td> 2.9</td>\n",
       "      <td> 6.3</td>\n",
       "      <td> 1.8</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td> 5.6</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 4.9</td>\n",
       "      <td> 2.0</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "105                7.6               3.0                6.6               2.1   \n",
       "75                 6.6               3.0                4.4               1.4   \n",
       "89                 5.5               2.5                4.0               1.3   \n",
       "40                 5.0               3.5                1.3               0.3   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "76                 6.8               2.8                4.8               1.4   \n",
       "41                 4.5               2.3                1.3               0.3   \n",
       "133                6.3               2.8                5.1               1.5   \n",
       "26                 5.0               3.4                1.6               0.4   \n",
       "125                7.2               3.2                6.0               1.8   \n",
       "129                7.2               3.0                5.8               1.6   \n",
       "78                 6.0               2.9                4.5               1.5   \n",
       "87                 6.3               2.3                4.4               1.3   \n",
       "112                6.8               3.0                5.5               2.1   \n",
       "57                 4.9               2.4                3.3               1.0   \n",
       "93                 5.0               2.3                3.3               1.0   \n",
       "69                 5.6               2.5                3.9               1.1   \n",
       "99                 5.7               2.8                4.1               1.3   \n",
       "114                5.8               2.8                5.1               2.4   \n",
       "92                 5.8               2.6                4.0               1.2   \n",
       "38                 4.4               3.0                1.3               0.2   \n",
       "132                6.4               2.8                5.6               2.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "56                 6.3               3.3                4.7               1.6   \n",
       "101                5.8               2.7                5.1               1.9   \n",
       "13                 4.3               3.0                1.1               0.1   \n",
       "94                 5.6               2.7                4.2               1.3   \n",
       "138                6.0               3.0                4.8               1.8   \n",
       "21                 5.1               3.7                1.5               0.4   \n",
       "139                6.9               3.1                5.4               2.1   \n",
       "..                 ...               ...                ...               ...   \n",
       "64                 5.6               2.9                3.6               1.3   \n",
       "24                 4.8               3.4                1.9               0.2   \n",
       "44                 5.1               3.8                1.9               0.4   \n",
       "35                 5.0               3.2                1.2               0.2   \n",
       "127                6.1               3.0                4.9               1.8   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "47                 4.6               3.2                1.4               0.2   \n",
       "96                 5.7               2.9                4.2               1.3   \n",
       "67                 5.8               2.7                4.1               1.0   \n",
       "36                 5.5               3.5                1.3               0.2   \n",
       "130                7.4               2.8                6.1               1.9   \n",
       "33                 5.5               4.2                1.4               0.2   \n",
       "88                 5.6               3.0                4.1               1.3   \n",
       "55                 5.7               2.8                4.5               1.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "141                6.9               3.1                5.1               2.3   \n",
       "128                6.4               2.8                5.6               2.1   \n",
       "108                6.7               2.5                5.8               1.8   \n",
       "62                 6.0               2.2                4.0               1.0   \n",
       "34                 4.9               3.1                1.5               0.1   \n",
       "53                 5.5               2.3                4.0               1.3   \n",
       "104                6.5               3.0                5.8               2.2   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "109                7.2               3.6                6.1               2.5   \n",
       "31                 5.4               3.4                1.5               0.4   \n",
       "137                6.4               3.1                5.5               1.8   \n",
       "103                6.3               2.9                5.6               1.8   \n",
       "107                7.3               2.9                6.3               1.8   \n",
       "121                5.6               2.8                4.9               2.0   \n",
       "\n",
       "     Target  \n",
       "105       2  \n",
       "75        1  \n",
       "89        1  \n",
       "40        0  \n",
       "18        0  \n",
       "76        1  \n",
       "41        0  \n",
       "133       2  \n",
       "26        0  \n",
       "125       2  \n",
       "129       2  \n",
       "78        1  \n",
       "87        1  \n",
       "112       2  \n",
       "57        1  \n",
       "93        1  \n",
       "69        1  \n",
       "99        1  \n",
       "114       2  \n",
       "92        1  \n",
       "38        0  \n",
       "132       2  \n",
       "4         0  \n",
       "56        1  \n",
       "101       2  \n",
       "13        0  \n",
       "94        1  \n",
       "138       2  \n",
       "21        0  \n",
       "139       2  \n",
       "..      ...  \n",
       "64        1  \n",
       "24        0  \n",
       "44        0  \n",
       "35        0  \n",
       "127       2  \n",
       "148       2  \n",
       "47        0  \n",
       "96        1  \n",
       "67        1  \n",
       "36        0  \n",
       "130       2  \n",
       "33        0  \n",
       "88        1  \n",
       "55        1  \n",
       "146       2  \n",
       "145       2  \n",
       "141       2  \n",
       "128       2  \n",
       "108       2  \n",
       "62        1  \n",
       "34        0  \n",
       "53        1  \n",
       "104       2  \n",
       "149       2  \n",
       "109       2  \n",
       "31        0  \n",
       "137       2  \n",
       "103       2  \n",
       "107       2  \n",
       "121       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.iloc[data_indices,]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. EXTRA CREDIT 1: Using the value of K obtained in (3) above, vary the number of folds used for cross-validation across an interesting range, e.g. [ 2, 3, 5, 6, 10, 15]. How does classifier accuracy vary with the number of folds used? Do you think there exists an optimal number of folds to use for this particular problem? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_folds = [2, 3, 5, 6, 10, 15]\n",
    "xvalidate_scores = []\n",
    "for f in n_folds:\n",
    "    scores = xvalidate(iris.data, iris.target, 12, f)\n",
    "    xvalidate_scores.append(mean(scores))\n",
    "    \n",
    "plt.plot(n_neighbors, xvalidate_scores)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classifier accuracy is optimal at 5 folds.  Fewer folds than this results in less accuracy and more folds results in less accuracy too.  There is a steep drop as number of folds is decreased from 5 where increasing the number of folds only results in a gradual loss of accuracy.  The change in accuracy between all the tested fold values is not drastic.  Since 5 folds results in the best classifier accuracy, it is clearly the optimal value."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. EXTRA CREDIT 2: Write your own implementation of KNN classification in Python, without using the methods from sklearn. Compare your results with the results you obtained using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.88\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# compute distance between two N-dimensional points\n",
    "def dist(data1, data2):\n",
    "    tot = 0\n",
    "    \n",
    "    # loop over each dimension in the data\n",
    "    for i in range(len(data1)):\n",
    "        # append distance value for a single dimension\n",
    "        d = data1[i] - data2[i]\n",
    "        tot += math.pow(d, 2)\n",
    "        \n",
    "    # return square root of sums\n",
    "    return math.sqrt(tot)\n",
    "\n",
    "# get the label of a point by looking at the classification of its K closest neighbors\n",
    "def get_label(data, labels, point_index, k):\n",
    "        \n",
    "    # get the point at the index\n",
    "    point = data[point_index]\n",
    "        \n",
    "    # get the distances between this point and every other one\n",
    "    distances = {}\n",
    "    for i in range(len(data)):\n",
    "        if (i != point_index):\n",
    "            distances[i] = dist(data[i], point)\n",
    "        \n",
    "    # sort the distances\n",
    "    sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "         \n",
    "    # get the data indices of the closest K points, ignoring distance 0 which is this point! \n",
    "    indices = []\n",
    "    for j in range(k+1):\n",
    "        if (sorted_distances[j][1] != 0.):\n",
    "            indices.append(sorted_distances[j][0])\n",
    "            \n",
    "    # get the labels of these points\n",
    "    neighbor_labels = []\n",
    "    for i in indices:\n",
    "        neighbor_labels.append(labels[i])\n",
    "                \n",
    "    # count the label values\n",
    "    label_counts = collections.Counter(neighbor_labels)\n",
    "    sorted_counts = sorted(label_counts.items(), key=operator.itemgetter(1))\n",
    "        \n",
    "    # return the label with the most counts\n",
    "    return sorted_counts[0][0]\n",
    "        \n",
    "def my_knn(values, target, k):\n",
    "    labels = []\n",
    "    # loop over the values\n",
    "    for i in range(len(values)):\n",
    "        \n",
    "        # get the features which will be classified\n",
    "        value = values[i]\n",
    "        \n",
    "        # get the label for a single value \n",
    "        label = get_label(values, target, i, k)\n",
    "        \n",
    "        # append the label for this point\n",
    "        labels.append(label)\n",
    "\n",
    "    # return all label values for the inputs\n",
    "    return labels\n",
    "\n",
    "# compute score by comparing test and train data\n",
    "def compute_score(test, train):\n",
    "    nright = 0\n",
    "    for i in range(len(test)):\n",
    "        if (test[i] == train[i]):\n",
    "            nright += 1\n",
    "    return nright / len(test)\n",
    "\n",
    "# get the labels from running my KNN method (example of just running on all data and labels\n",
    "# without using stratification)\n",
    "labels = my_knn(iris.data, iris.target, 5)\n",
    "\n",
    "#print \"labels\", labels\n",
    "\n",
    "score = compute_score(labels, iris.target)\n",
    "print \"score\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python imports\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn import neighbors, datasets, feature_selection\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "\n",
    "import collections\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "iris_df['Target'] = iris.target"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Implement KNN classification, using the sklearn package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10b83b750>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2QBwsDczGwD/H176Y0AZFsBgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1030a4c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split data and target classifications into train and test sets and run KNN classification\n",
    "def knn(data, target, n_neighbors):\n",
    "\n",
    "    # split the data into train and test datasets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=12)\n",
    "\n",
    "    # Loop through each K value, run KNN classification, and append the scores to the output\n",
    "    scores = []\n",
    "    for n in n_neighbors:\n",
    "        clf = neighbors.KNeighborsClassifier(n)\n",
    "        clf.fit(x_train, y_train)\n",
    "        scores.append(clf.score(x_test, y_test))\n",
    "        \n",
    "    return scores\n",
    "\n",
    "# use odd for K from 1 to 51\n",
    "n_neighbors = range(1, 51, 2)\n",
    "\n",
    "# get the scores\n",
    "scores = knn(iris.data, iris.target, n_neighbors)\n",
    "\n",
    "# plot result as K vs score\n",
    "plt.plot(n_neighbors, scores, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. Use the sklearn package to implement cross-validation for your classifier. Use 5 folds for your cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96666667  0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.96\n",
      "[ 0.96666667  0.96666667  0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.96666667  1.          0.93333333  0.96666667  1.        ]\n",
      "0.973333333333\n",
      "[ 0.96666667  1.          0.96666667  0.96666667  1.        ]\n",
      "0.98\n",
      "[ 0.96666667  1.          0.96666667  0.93333333  1.        ]\n",
      "0.973333333333\n",
      "[ 0.93333333  1.          1.          0.96666667  1.        ]\n",
      "0.98\n",
      "[ 0.93333333  1.          0.96666667  0.96666667  1.        ]\n",
      "0.973333333333\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.96666667  1.        ]\n",
      "0.966666666667\n",
      "[ 0.93333333  1.          0.93333333  0.93333333  1.        ]\n",
      "0.96\n",
      "[ 0.93333333  0.96666667  0.93333333  0.96666667  1.        ]\n",
      "0.96\n",
      "[ 0.9         0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.946666666667\n",
      "[ 0.9         0.96666667  0.9         0.9         1.        ]\n",
      "0.933333333333\n",
      "[ 0.9         0.96666667  0.9         0.9         1.        ]\n",
      "0.933333333333\n",
      "[ 0.93333333  0.96666667  0.9         0.9         1.        ]\n",
      "0.94\n",
      "[ 0.93333333  0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.953333333333\n",
      "[ 0.9         0.96666667  0.93333333  0.93333333  1.        ]\n",
      "0.946666666667\n",
      "[ 0.9         0.93333333  0.93333333  0.93333333  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.93333333  0.93333333  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.93333333  0.93333333  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.9         0.96666667  1.        ]\n",
      "0.94\n",
      "[ 0.9         0.93333333  0.86666667  0.96666667  0.96666667]\n",
      "0.926666666667\n",
      "[ 0.9         0.93333333  0.9         0.96666667  1.        ]\n",
      "0.94\n"
     ]
    }
   ],
   "source": [
    "# run 5-fold cross-validation on the input and targets and return the scores\n",
    "def xvalidate(data, target, n_neighbor, n_folds):\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbor, weights='uniform')\n",
    "    clf.fit(data, target)\n",
    "    scores = cross_val_score(clf, data, target, cv=n_folds)\n",
    "    return scores\n",
    "\n",
    "# simple util method for getting the mean value of a list\n",
    "def mean(values):\n",
    "    tot = 0\n",
    "    for v in values:\n",
    "        tot += v\n",
    "    return tot / len(values)\n",
    "\n",
    "# run cross-validation using a series of K values\n",
    "for x in range(1, 51, 2):\n",
    "    xvalidate_scores = xvalidate(iris.data, iris.target, x, 5)\n",
    "    print xvalidate_scores\n",
    "    print mean(xvalidate_scores)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Use your KNN classifier and cross-validation code from (1) and (2) above to determine the optimal value of K (number of nearest neighbors to consult) for this Iris dataset. Hint: This hyperparameter will be a number between 1 and 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "xvalidate() takes exactly 4 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0591a3d2cb70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_optimal_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"neighbor scores\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"optimal K =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0591a3d2cb70>\u001b[0m in \u001b[0;36mfind_optimal_k\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mneighbor_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mxvalidate_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalidate_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mneighbor_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: xvalidate() takes exactly 4 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "print len(iris.data)\n",
    "\n",
    "# find the optimal K which is the highest value of K with the best score\n",
    "# determined by 5-fold cross-validation on a list of odd-numbered K from 1 to 51\n",
    "def find_optimal_k(data):\n",
    "    max_neighbors = int(len(data) / 2)\n",
    "    neighbor_scores = {}\n",
    "    for x in range(1, max_neighbors, 1):\n",
    "        xvalidate_scores = xvalidate(iris.data, iris.target, x)        \n",
    "        avg = mean(xvalidate_scores)\n",
    "        neighbor_scores[x] = avg\n",
    "    max_value = max(neighbor_scores.values())\n",
    "    indices = [i for i, x in enumerate(neighbor_scores.values()) if x == max_value]\n",
    "    max_index = indices[-1] + 1 # add 1 from 0-based indexing\n",
    "    return max_index, neighbor_scores\n",
    "\n",
    "k, neighbor_scores = find_optimal_k(iris.data)\n",
    "print \"neighbor scores\", neighbor_scores\n",
    "print \"optimal K =\", k"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Using matplotlib, plot classifier accuracy versus the hyperparameter K for a range of K that you consider interesting. Explain in words what you are seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(neighbor_scores.keys(), neighbor_scores.values())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There is a \"tie\" for the optimal value of K between 6 to 12.  Several K values in this range scored the same (at about 0.98).  The optimal value of K is 12 if we consider \"optimal\" to be the highest value of K with the highest score (in case of a tie).  As K is increased and more neighbors are added to the classication, the accuracy decreases, due to the majority classification value tending to dominate as locality decreases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Now, write your own implementation of cross-validation in Python without using the cross-validation methods from sklearn. Cross validation is a very important concept. Implementing it yourself in Python is the best way to learn and understand it. Compare the results of your cross-validation code with your results using the cross-validation in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def my_xvalidate(data, target, folds, n_neighbors):\n",
    "    \n",
    "    # make a list of the indices in the dataset\n",
    "    data_indices = range(0, len(data), 1)\n",
    "    \n",
    "    # shuffle the indices\n",
    "    random.shuffle(data_indices)\n",
    "    \n",
    "    # get the number of items in each fold\n",
    "    n_items = int((len(data) / folds))\n",
    "    \n",
    "    # start index for fold\n",
    "    begin_index = 0\n",
    "    \n",
    "    # end index for fold\n",
    "    end_index = n_items - 1\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # iterate over the folds\n",
    "    for n_fold in range(folds):\n",
    "        \n",
    "        # get the data for this fold\n",
    "        fold_data = data[begin_index:end_index, :]\n",
    "        \n",
    "        # get the labels for this fold\n",
    "        fold_labels = target[begin_index:end_index]\n",
    "        \n",
    "        # score this fold\n",
    "        fold_score = knn(fold_data, fold_labels, n_neighbors)\n",
    "        \n",
    "        # append the score for this fold\n",
    "        scores.append(fold_score[0])\n",
    "        \n",
    "        # increment begin and end indices for next fold\n",
    "        begin_index += n_items\n",
    "        end_index += n_items    \n",
    "    return scores\n",
    "    \n",
    "# run cross-validation on all iris data using 5-fold cross-validation with K = 11\n",
    "scores = my_xvalidate(iris.data, iris.target, 5, [11])\n",
    "print scores\n",
    "print mean(scores)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. EXTRA CREDIT 1: Using the value of K obtained in (3) above, vary the number of folds used for cross-validation across an interesting range, e.g. [ 2, 3, 5, 6, 10, 15]. How does classifier accuracy vary with the number of folds used? Do you think there exists an optimal number of folds to use for this particular problem? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_folds = [2, 3, 5, 6, 10, 15]\n",
    "xvalidate_scores = []\n",
    "for f in n_folds:\n",
    "    scores = xvalidate(iris.data, iris.target, 12, f)\n",
    "    xvalidate_scores.append(mean(scores))\n",
    "    \n",
    "plt.plot(n_neighbors, xvalidate_scores)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classifier accuracy is optimal at 5 folds.  Fewer folds than this results in less accuracy and more folds results in less accuracy too.  There is a steep drop as number of folds is decreased from 5 where increasing the number of folds only results in a gradual loss of accuracy.  The change in accuracy between all the tested fold values is not drastic.  Since 5 folds results in the best classifier accuracy, it is clearly the optimal value."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. EXTRA CREDIT 2: Write your own implementation of KNN classification in Python, without using the methods from sklearn. Compare your results with the results you obtained using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.88\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# compute distance between two N-dimensional points\n",
    "def dist(data1, data2):\n",
    "    tot = 0\n",
    "    \n",
    "    # loop over each dimension in the data\n",
    "    for i in range(len(data1)):\n",
    "        # append distance value for a single dimension\n",
    "        d = data1[i] - data2[i]\n",
    "        tot += math.pow(d, 2)\n",
    "        \n",
    "    # return square root of sums\n",
    "    return math.sqrt(tot)\n",
    "\n",
    "# get the label of a point by looking at the classification of its K closest neighbors\n",
    "def get_label(data, labels, point_index, k):\n",
    "        \n",
    "    # get the point at the index\n",
    "    point = data[point_index]\n",
    "        \n",
    "    # get the distances between this point and every other one\n",
    "    distances = {}\n",
    "    for i in range(len(data)):\n",
    "        if (i != point_index):\n",
    "            distances[i] = dist(data[i], point)\n",
    "        \n",
    "    # sort the distances\n",
    "    sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "         \n",
    "    # get the data indices of the closest K points, ignoring distance 0 which is this point! \n",
    "    indices = []\n",
    "    for j in range(k+1):\n",
    "        if (sorted_distances[j][1] != 0.):\n",
    "            indices.append(sorted_distances[j][0])\n",
    "            \n",
    "    # get the labels of these points\n",
    "    neighbor_labels = []\n",
    "    for i in indices:\n",
    "        neighbor_labels.append(labels[i])\n",
    "                \n",
    "    # count the label values\n",
    "    label_counts = collections.Counter(neighbor_labels)\n",
    "    sorted_counts = sorted(label_counts.items(), key=operator.itemgetter(1))\n",
    "        \n",
    "    # return the label with the most counts\n",
    "    return sorted_counts[0][0]\n",
    "        \n",
    "def my_knn(values, target, k):\n",
    "    labels = []\n",
    "    # loop over the values\n",
    "    for i in range(len(values)):\n",
    "        \n",
    "        # get the features which will be classified\n",
    "        value = values[i]\n",
    "        \n",
    "        # get the label for a single value \n",
    "        label = get_label(values, target, i, k)\n",
    "        \n",
    "        # append the label for this point\n",
    "        labels.append(label)\n",
    "\n",
    "    # return all label values for the inputs\n",
    "    return labels\n",
    "\n",
    "# compute score by comparing test and train data\n",
    "def compute_score(test, train):\n",
    "    nright = 0\n",
    "    for i in range(len(test)):\n",
    "        if (test[i] == train[i]):\n",
    "            nright += 1\n",
    "    return nright / len(test)\n",
    "\n",
    "# get the labels from running my KNN method (example of just running on all data and labels\n",
    "# without using stratification)\n",
    "labels = my_knn(iris.data, iris.target, 5)\n",
    "\n",
    "#print \"labels\", labels\n",
    "\n",
    "score = compute_score(labels, iris.target)\n",
    "print \"score\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
